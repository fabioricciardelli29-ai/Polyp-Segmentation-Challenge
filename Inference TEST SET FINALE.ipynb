{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wfbQdhdUbz7r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736864260446,"user_tz":-60,"elapsed":24850,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"d473ca00-f3e3-458a-a0e8-2a3ddc57e619"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Collegamento a Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5lohijhb7pw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736864399358,"user_tz":-60,"elapsed":138917,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"a9405276-5e26-45d2-df91-011b31df9435"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","peft 0.14.0 requires torch>=1.13.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.3/452.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n","!pip install -q mmcv==2.0.0rc4 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-SZ6AjbcDok","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736864417789,"user_tz":-60,"elapsed":18449,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"3eea7cfa-29fc-47a4-d952-6ceae5f144cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q mmsegmentation==1.2.2\n","!pip install -q mmengine==0.10.3\n","!pip install -q ftfy==6.2.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPSgmvzNcHOn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736864420000,"user_tz":-60,"elapsed":2216,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"292700f5-c666-4263-c1c4-72c5d2779582"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.11.0+cu113 True\n","1.2.2\n"]}],"source":["# Check Pytorch installation\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","\n","# Check MMSegmentation installation\n","import mmseg\n","print(mmseg.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maDgbYbMcSbN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736864433163,"user_tz":-60,"elapsed":13169,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"61ef3e38-ab2e-456f-83a0-a426e33f7083"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}],"source":["import os\n","import numpy as np\n","import torch\n","import json\n","import re\n","from tqdm import tqdm\n","from PIL import Image\n","from skimage import morphology, measure, color, img_as_ubyte, img_as_float\n","from skimage.color import rgb2gray\n","from scipy import ndimage\n","from scipy.ndimage import label\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from mmseg.apis import init_model, inference_model\n","from mmseg.registry import DATASETS\n","from mmseg.datasets import BaseSegDataset\n","from mmengine import Config\n","from mmengine.config import Config\n"]},{"cell_type":"code","source":["#creazione per fare inference con il test riorganizzato come lo hanno i professori\n","def reorganize_test_data():\n","    main_folder = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_DATASET_INFERENCE/TEST'\n","    output_folder = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_SOLO_TEST_riorg'\n","\n","    # Percorsi di destinazione per immagini e maschere\n","    dest_images_folder = os.path.join(output_folder, 'images')\n","    dest_masks_folder = os.path.join(output_folder, 'masks')\n","\n","    # Crea le cartelle di destinazione se non esistono\n","    os.makedirs(dest_images_folder, exist_ok=True)\n","    os.makedirs(dest_masks_folder, exist_ok=True)\n","\n","    # Ottieni automaticamente i nomi delle sottocartelle presenti in main_folder\n","    sottocartelle = [d for d in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, d))]\n","\n","    # Processa ciascuna sottocartella identificata\n","    for seq in sottocartelle:\n","        # Percorsi delle cartelle di origine\n","        src_images_folder = os.path.join(main_folder, seq, 'images')\n","        src_masks_folder = os.path.join(main_folder, seq, 'masks')\n","\n","        # Controlla se le cartelle 'images' e 'masks' esistono per la sequenza\n","        if not os.path.exists(src_images_folder+'_'+seq) or not os.path.exists(src_masks_folder+'_'+seq):\n","            print(f\"Le cartelle 'images' o 'masks' non esistono per la sequenza {seq}. Skipping...\")\n","            continue\n","\n","        # Percorsi delle cartelle di destinazione per ogni sequenza\n","        dest_images_seq_folder = os.path.join(dest_images_folder, seq)\n","        dest_masks_seq_folder = os.path.join(dest_masks_folder, seq)\n","\n","        # Crea le cartelle di destinazione per la sequenza\n","        os.makedirs(dest_images_seq_folder, exist_ok=True)\n","        os.makedirs(dest_masks_seq_folder, exist_ok=True)\n","\n","        # Sposta le immagini nella nuova struttura\n","        for image_name in os.listdir(src_images_folder+'_'+seq):\n","            src_image_path = os.path.join(src_images_folder+'_'+seq, image_name)\n","            dest_image_path = os.path.join(dest_images_seq_folder, image_name)\n","            try:\n","                image = Image.open(src_image_path)\n","                image.save(dest_image_path)\n","            except Exception as e:\n","                print(f\"Errore durante il salvataggio dell'immagine {image_name}: {e}\")\n","\n","        # Sposta le maschere nella nuova struttura\n","        for mask_name in os.listdir(src_masks_folder+'_'+seq):\n","            src_mask_path = os.path.join(src_masks_folder+'_'+seq, mask_name)\n","            dest_mask_path = os.path.join(dest_masks_seq_folder, mask_name)\n","            try:\n","                mask = Image.open(src_mask_path)\n","                mask.save(dest_mask_path)\n","            except Exception as e:\n","                print(f\"Errore durante il salvataggio della maschera {mask_name}: {e}\")\n","\n","        print(f\"Riorganizzata la sequenza {seq}\")\n","\n","    print(\"Riorganizzazione completata.\")\n","\n","# Esegui la funzione\n","reorganize_test_data()\n"],"metadata":{"id":"1u7GtWLf9FCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BN1Rok531cSv"},"outputs":[],"source":["####################################\n","#### DO NOT CHANGE THIS CELL #######\n","####################################\n","import os\n","working_folder = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA'\n","checkpoint_path = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/weights_unet_dice_loss_cambio_parametri'\n","\n","# Results folder\n","results_folder  = \"/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/results_Cambio_parametri_dice_loss_TEST\"\n","\n","# Data folders\n","test_img_folder = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_SOLO_TEST_riorg/images'\n","gt_mask_folder  = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_SOLO_TEST_riorg/masks'\n","\n","if not os.path.exists(results_folder):\n","    os.makedirs(results_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTMeuNjmcp0T"},"outputs":[],"source":["####################################\n","#### DO NOT CHANGE THIS CELL #######\n","####################################\n","\n","# Classes and palette for VERSE dataset\n","classes = ('background', 'object')\n","palettePOLYP = [\n","    (0, 0, 0),       # background - black\n","    (255, 255, 255)  # object - white\n","]\n","\n","@DATASETS.register_module()\n","class POLYP(BaseSegDataset):\n","    METAINFO = dict(classes=classes, palette=palettePOLYP)\n","\n","    def __init__(self, **kwargs):\n","        super().__init__(img_suffix='.png',\n","                         seg_map_suffix='.png',\n","                         reduce_zero_label=False,\n","                         **kwargs)"]},{"cell_type":"code","source":["####################################\n","#### DO NOT CHANGE THIS CELL #######\n","####################################\n","\n","\n","config_file = os.path.join(checkpoint_path,'unet-s5-d16_fcn_4xb4-160k_cityscapes-512x1024.py')\n","cfg = Config.fromfile(config_file)\n","cfg.device='cuda'\n","\n","# Visualizzazione del config caricato (unet)\n","print(f'Config:\\n{cfg.pretty_text}')\n","checkpoint_path=os.path.join(checkpoint_path,'iter_2961.pth')\n","# Initialize the model from the config and the checkpoint\n","model = init_model(cfg, checkpoint_path, 'cuda:0')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wd9ZZ-IsTCuX","executionInfo":{"status":"ok","timestamp":1736864634739,"user_tz":-60,"elapsed":27489,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"5695d2b8-4c0b-4a50-b68d-276ef9bba48a","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Config:\n","crop_size = (\n","    256,\n","    256,\n",")\n","data_preprocessor = dict(\n","    bgr_to_rgb=True,\n","    mean=[\n","        123.675,\n","        116.28,\n","        103.53,\n","    ],\n","    pad_val=0,\n","    seg_pad_val=255,\n","    size=(\n","        256,\n","        256,\n","    ),\n","    std=[\n","        58.395,\n","        57.12,\n","        57.375,\n","    ],\n","    type='SegDataPreProcessor')\n","data_root = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_SPLIT_No_organize_CROP_CON_0+ INPAINTING_Normalized'\n","dataset_type = 'POLYP'\n","default_hooks = dict(\n","    checkpoint=dict(by_epoch=False, interval=329, type='CheckpointHook'),\n","    logger=dict(interval=50, log_metric_by_epoch=False, type='LoggerHook'),\n","    param_scheduler=dict(type='ParamSchedulerHook'),\n","    sampler_seed=dict(type='DistSamplerSeedHook'),\n","    timer=dict(type='IterTimerHook'),\n","    visualization=dict(type='SegVisualizationHook'))\n","default_scope = 'mmseg'\n","device = 'cuda'\n","env_cfg = dict(\n","    cudnn_benchmark=True,\n","    dist_cfg=dict(backend='nccl'),\n","    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n","img_ratios = [\n","    0.95,\n","    1.0,\n","    1.05,\n","]\n","launcher = 'none'\n","load_from = '/content/drive/MyDrive/CHALLENGE/checkpoints/fcn_unet_s5-d16_4x4_512x1024_160k_cityscapes_20211210_145204-6860854e.pth'\n","log_level = 'INFO'\n","log_processor = dict(by_epoch=False)\n","model = dict(\n","    auxiliary_head=dict(\n","        align_corners=False,\n","        channels=64,\n","        concat_input=False,\n","        dropout_ratio=0.1,\n","        in_channels=128,\n","        in_index=3,\n","        loss_decode=[\n","            dict(loss_weight=0.2, type='DiceLoss', use_sigmoid=False),\n","        ],\n","        norm_cfg=dict(requires_grad=True, type='BN'),\n","        num_classes=2,\n","        num_convs=1,\n","        type='FCNHead'),\n","    backbone=dict(\n","        act_cfg=dict(type='ReLU'),\n","        base_channels=64,\n","        conv_cfg=None,\n","        dec_dilations=(\n","            1,\n","            1,\n","            1,\n","            1,\n","        ),\n","        dec_num_convs=(\n","            2,\n","            2,\n","            2,\n","            2,\n","        ),\n","        downsamples=(\n","            True,\n","            True,\n","            True,\n","            True,\n","        ),\n","        enc_dilations=(\n","            1,\n","            1,\n","            1,\n","            1,\n","            1,\n","        ),\n","        enc_num_convs=(\n","            2,\n","            2,\n","            2,\n","            2,\n","            2,\n","        ),\n","        in_channels=3,\n","        norm_cfg=dict(requires_grad=True, type='BN'),\n","        norm_eval=False,\n","        num_stages=5,\n","        strides=(\n","            1,\n","            1,\n","            1,\n","            1,\n","            1,\n","        ),\n","        type='UNet',\n","        upsample_cfg=dict(type='InterpConv'),\n","        with_cp=False),\n","    data_preprocessor=dict(\n","        bgr_to_rgb=True,\n","        mean=[\n","            123.675,\n","            116.28,\n","            103.53,\n","        ],\n","        pad_val=0,\n","        seg_pad_val=255,\n","        size=(\n","            256,\n","            256,\n","        ),\n","        std=[\n","            58.395,\n","            57.12,\n","            57.375,\n","        ],\n","        type='SegDataPreProcessor'),\n","    decode_head=dict(\n","        align_corners=False,\n","        channels=64,\n","        concat_input=False,\n","        dropout_ratio=0.1,\n","        in_channels=64,\n","        in_index=4,\n","        loss_decode=[\n","            dict(loss_weight=1.0, type='DiceLoss', use_sigmoid=False),\n","        ],\n","        norm_cfg=dict(requires_grad=True, type='BN'),\n","        num_classes=2,\n","        num_convs=1,\n","        type='FCNHead'),\n","    pretrained=None,\n","    test_cfg=dict(crop_size=256, mode='whole', stride=170),\n","    train_cfg=None,\n","    type='EncoderDecoder')\n","norm_cfg = dict(requires_grad=True, type='BN')\n","optim_wrapper = dict(\n","    clip_grad=None,\n","    optimizer=dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005),\n","    type='OptimWrapper')\n","optimizer = dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005)\n","param_scheduler = [\n","    dict(\n","        begin=0,\n","        by_epoch=False,\n","        end=160000,\n","        eta_min=0.0001,\n","        power=0.9,\n","        type='PolyLR'),\n","]\n","resume = False\n","test_cfg = dict(type='TestLoop')\n","test_dataloader = dict(\n","    batch_size=1,\n","    dataset=dict(\n","        data_prefix=dict(img_path='TEST/images', seg_map_path='TEST/masks'),\n","        data_root=\n","        '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_SPLIT_No_organize_CROP_CON_0+ INPAINTING_Normalized',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(keep_ratio=True, scale=(\n","                256,\n","                256,\n","            ), type='Resize'),\n","            dict(reduce_zero_label=False, type='LoadAnnotations'),\n","            dict(type='PackSegInputs'),\n","        ],\n","        type='POLYP'),\n","    num_workers=4,\n","    persistent_workers=True,\n","    sampler=dict(shuffle=False, type='DefaultSampler'))\n","test_evaluator = dict(\n","    iou_metrics=[\n","        'mIoU',\n","    ], type='IoUMetric')\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(keep_ratio=True, scale=(\n","        256,\n","        256,\n","    ), type='Resize'),\n","    dict(reduce_zero_label=False, type='LoadAnnotations'),\n","    dict(type='PackSegInputs'),\n","]\n","train_cfg = dict(max_iters=6580, type='IterBasedTrainLoop', val_interval=329)\n","train_dataloader = dict(\n","    batch_size=8,\n","    dataset=dict(\n","        data_prefix=dict(img_path='TRAIN/images', seg_map_path='TRAIN/masks'),\n","        data_root=\n","        '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_SPLIT_No_organize_CROP_CON_0+ INPAINTING_Normalized',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(reduce_zero_label=False, type='LoadAnnotations'),\n","            dict(\n","                keep_ratio=True,\n","                ratio_range=(\n","                    0.95,\n","                    1.05,\n","                ),\n","                scale=(\n","                    256,\n","                    256,\n","                ),\n","                type='RandomResize'),\n","            dict(\n","                cat_max_ratio=0.95, crop_size=(\n","                    256,\n","                    256,\n","                ), type='RandomCrop'),\n","            dict(prob=0.5, type='RandomFlip'),\n","            dict(type='PhotoMetricDistortion'),\n","            dict(type='PackSegInputs'),\n","        ],\n","        type='POLYP'),\n","    num_workers=4,\n","    persistent_workers=True,\n","    sampler=dict(shuffle=True, type='InfiniteSampler'))\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(reduce_zero_label=False, type='LoadAnnotations'),\n","    dict(\n","        keep_ratio=True,\n","        ratio_range=(\n","            0.95,\n","            1.05,\n","        ),\n","        scale=(\n","            256,\n","            256,\n","        ),\n","        type='RandomResize'),\n","    dict(cat_max_ratio=0.95, crop_size=(\n","        256,\n","        256,\n","    ), type='RandomCrop'),\n","    dict(prob=0.5, type='RandomFlip'),\n","    dict(type='PhotoMetricDistortion'),\n","    dict(type='PackSegInputs'),\n","]\n","tta_model = dict(type='SegTTAModel')\n","tta_pipeline = [\n","    dict(backend_args=None, type='LoadImageFromFile'),\n","    dict(\n","        transforms=[\n","            [\n","                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),\n","                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),\n","                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),\n","                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),\n","                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),\n","                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),\n","            ],\n","            [\n","                dict(direction='horizontal', prob=0.0, type='RandomFlip'),\n","                dict(direction='horizontal', prob=1.0, type='RandomFlip'),\n","            ],\n","            [\n","                dict(type='LoadAnnotations'),\n","            ],\n","            [\n","                dict(type='PackSegInputs'),\n","            ],\n","        ],\n","        type='TestTimeAug'),\n","]\n","val_cfg = dict(type='ValLoop')\n","val_dataloader = dict(\n","    batch_size=8,\n","    dataset=dict(\n","        data_prefix=dict(img_path='VALID/images', seg_map_path='VALID/masks'),\n","        data_root=\n","        '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/PolypGen-EIM-24-25_SPLIT_No_organize_CROP_CON_0+ INPAINTING_Normalized',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(keep_ratio=True, scale=(\n","                256,\n","                256,\n","            ), type='Resize'),\n","            dict(reduce_zero_label=False, type='LoadAnnotations'),\n","            dict(type='PackSegInputs'),\n","        ],\n","        type='POLYP'),\n","    num_workers=4,\n","    persistent_workers=True,\n","    sampler=dict(shuffle=False, type='DefaultSampler'))\n","val_evaluator = dict(\n","    iou_metrics=[\n","        'mIoU',\n","    ], type='IoUMetric')\n","vis_backends = [\n","    dict(type='LocalVisBackend'),\n","]\n","visualizer = dict(\n","    classes=(\n","        'background',\n","        'object',\n","    ),\n","    name='visualizer',\n","    palette=[\n","        [\n","            0,\n","            0,\n","            0,\n","        ],\n","        [\n","            255,\n","            255,\n","            255,\n","        ],\n","    ],\n","    save_dir=\n","    '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/weights_unet_dice_loss_cambio_parametri',\n","    type='SegLocalVisualizer',\n","    vis_backends=[\n","        dict(type='LocalVisBackend'),\n","    ])\n","work_dir = '/content/drive/MyDrive/CHALLENGE_NUOVA_CARTELLA/weights_unet_dice_loss_cambio_parametri'\n","workflow = [\n","    (\n","        'train',\n","        1,\n","    ),\n","    (\n","        'val',\n","        1,\n","    ),\n","]\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/mmseg/models/decode_heads/decode_head.py:120: UserWarning: For binary segmentation, we suggest using`out_channels = 1` to define the outputchannels of segmentor, and use `threshold`to convert `seg_logits` into a predictionapplying a threshold\n","  warnings.warn('For binary segmentation, we suggest using'\n","/usr/local/lib/python3.10/dist-packages/mmseg/models/builder.py:36: UserWarning: ``build_loss`` would be deprecated soon, please use ``mmseg.registry.MODELS.build()`` \n","  warnings.warn('``build_loss`` would be deprecated soon, please use '\n"]},{"output_type":"stream","name":"stdout","text":["Loads checkpoint by local backend from path: /content/drive/MyDrive/VERSIONE FINALE/RETE E PESI FINALI/iter_2961.pth\n"]}]},{"cell_type":"code","source":["\n","\n","# Ottieni la lista delle sequenze\n","seq_folders = os.listdir(test_img_folder)\n","\n","# Crea un dizionario per memorizzare le immagini per ogni sequenza\n","seq_dict = {}\n","\n","# Itera attraverso le cartelle delle sequenze\n","for seq in seq_folders:\n","    seq_images_folder = os.path.join(test_img_folder, seq)\n","\n","    # Verifica che la cartella esista e contenga immagini\n","    if os.path.isdir(seq_images_folder):\n","        # Ottieni la lista delle immagini in quella sequenza\n","        img_list = os.listdir(seq_images_folder)\n","\n","        # Verifica che la lista di immagini non sia vuota\n","        if img_list:\n","            for img_name in img_list:\n","                # Assicurati che l'immagine sia un file e non una sottocartella\n","                if os.path.isfile(os.path.join(seq_images_folder, img_name)):\n","                    # Aggiungi la sequenza nel dizionario se non è già presente\n","                    if seq not in seq_dict:\n","                        seq_dict[seq] = []\n","\n","                    # Aggiungi l'immagine alla lista della sequenza\n","                    seq_dict[seq].append(os.path.join(seq_images_folder, img_name))\n","\n","# Ordinamento delle immagini per ogni sequenza\n","for seq in seq_dict:\n","    seq_dict[seq].sort()\n","\n","# Ora seq_dict conterrà la lista delle immagini per ciascuna sequenza\n"],"metadata":{"id":"5wEcH8xeTE_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funzione per calcolare il Dice coefficient (dal codice 1)\n","def calculate_2d_dice(pred_image, gt_image):\n","    \"\"\"Calculate the 2D Dice coefficient between two images.\"\"\"\n","    # Flatten the images to 1D arrays\n","    pred_flat = pred_image.flatten()\n","    gt_flat = gt_image.flatten()\n","\n","\n","    # Handle the case where both masks are empty\n","    if np.sum(pred_flat) == 0 and np.sum(gt_flat) == 0:\n","        return 1.0  # Consider it a perfect match (both are empty)\n","\n","    # Compute the intersection between the prediction and the ground truth\n","    intersection = np.sum(pred_flat * gt_flat)\n","\n","    # Calculate the Dice coefficient\n","    dice = (2. * intersection) / (np.sum(pred_flat) + np.sum(gt_flat) + 1e-7)\n","\n","    return dice\n","\n","    # Funzione per estrarre l'ultimo numero dal nome del file\n","def extract_last_number_from_filename(filename):\n","    \"\"\"Estrae l'ultimo numero nel nome del file.\"\"\"\n","    # Trova tutti i numeri nel nome del file\n","    numbers = re.findall(r'\\d+', filename)\n","    # Restituisce l'ultimo numero trovato, se esiste, altrimenti 0\n","    return int(numbers[-1]) if numbers else 0\n","\n","def count_connected_objects(volume):\n","    \"\"\"Count the number of connected components in a binary volume.\"\"\"\n","    labeled_volume = measure.label(volume, connectivity=1)\n","    num_objects = np.max(labeled_volume)\n","    return num_objects\n","\n","def calculate_absolute_error(pred_count, gt_count):\n","    \"\"\"Calculate the absolute error in counting connected objects.\"\"\"\n","    error = abs(gt_count - pred_count)\n","    return error\n","\n","\n","# Funzione per calcolare il Temporal Stability Index (TSI)\n","def calculate_tsi(mask1, mask2):\n","    \"\"\"Calculate the Temporal Stability Index (TSI) between two binary masks.\"\"\"\n","    # Converte le maschere in formato binario e tipo float64\n","    mask1 = np.array(mask1, dtype=np.float64)\n","    mask2 = np.array(mask2, dtype=np.float64)\n","    area1 = np.sum(mask1)\n","    area2 = np.sum(mask2)\n","\n","    # Evita divisioni per zero\n","    if area1 + area2 == 0:\n","        return 0.0\n","\n","    # Calcola il TSI basato sulla variazione dell'area\n","    tsi = abs(area1 - area2) / (area1 + area2)\n","    return tsi\n","\n","def detect_and_remove_specular_highlights(image):\n","\n","    image_array = np.array(image)\n","    gray=rgb2gray(image_array)\n","    gray=(gray * 255).astype(np.uint8)\n","    _, mask = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n","    mask = cv2.dilate(mask, kernel, iterations=2)\n","    corrected_image = cv2.inpaint(image_array, mask, 5, cv2.INPAINT_TELEA)\n","\n","    return corrected_image, mask\n","\n","def regional_masking_and_inpainting(img, discontinuity_threshold=1 * 10**(-2)):\n","\n","        # Converti l'immagine in array numpy\n","        img_array = np.array(img)\n","\n","        # Converte in toni di grigio per calcolare i gradienti\n","        gray_image = color.rgb2gray(img_array)\n","        J = np.array(gray_image)\n","\n","        # Definire il kernel derivativo base\n","        hx = np.array([[-1, 0, 1]])\n","        hy = np.array([[-1], [0], [1]])\n","\n","        # Ricavare i gradienti orizzontale e verticale\n","        Jx = ndimage.correlate(J, hx)\n","        Jy = ndimage.correlate(J, hy)\n","\n","        # Identifica la riga centrale\n","        central_row = Jx[Jx.shape[0] // 2, :]\n","\n","        # Calcola le differenze tra pixel adiacenti\n","        diff = np.abs(np.diff(central_row))\n","\n","        # Trova il primo indice con discontinuità (da sinistra)\n","        left_bound = np.argmax(diff > discontinuity_threshold)\n","        # Trova il primo indice con discontinuità (da destra)\n","        right_bound = len(diff) - np.argmax(diff[::-1] > discontinuity_threshold) - 1\n","\n","        # Applica la modifica dei pixel sull'immagine a colori\n","        modified_image = img_array.copy()\n","        modified_image[:, :left_bound, :] = 16  # Imposta a 16 i pixel a sinistra di left_bound\n","        modified_image[:, right_bound:, :] = 16  # Imposta a 16 i pixel a destra di right_bound\n","\n","        # Reset a zero i pixel con tutti i canali pari a 16\n","        reset_image = modified_image.copy()\n","        mask = (reset_image[:, :, 0] == 16) & (reset_image[:, :, 1] == 16) & (reset_image[:, :, 2] == 16)\n","        reset_image[mask] = [0, 0, 0]  # Imposta a [0, 0, 0] i pixel corrispondenti\n","\n","        # Esegui l'inpainting sull'immagine croppata\n","        corrected_image, specular_mask = detect_and_remove_specular_highlights(reset_image)\n","\n","\n","        return corrected_image\n","\n","def normalizza_immagine(img_path):\n","    # Read grayscale image\n","    image = Image.open(img_path)\n","    image_array=np.array(image)\n","    # Normalize image\n","    normalized_image = cv2.normalize(\n","      image_array, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n","\n","    return Image.fromarray(normalized_image)\n","\n","\n","# Funzione per rimuovere regioni connesse minori di una soglia\n","def remove_small_regions(mask, min_size):\n","    # Identifica le regioni connesse\n","    labeled_mask, num_features = label(mask)\n","    # Crea una maschera vuota\n","    filtered_mask = np.zeros_like(mask, dtype=np.uint8)\n","    # Mantieni solo le regioni connesse maggiori della soglia\n","    for region_label in range(1, num_features + 1):\n","        region = (labeled_mask == region_label)\n","        if np.sum(region) >= min_size:\n","            filtered_mask[region] = 1\n","    return filtered_mask\n","\n"],"metadata":{"id":"Mr5JHiQuDXXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC-p8ENPc3VZ","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1736864731244,"user_tz":-60,"elapsed":96513,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"21f1fd63-f1d7-40b6-d4ee-a7443e94ec6e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing seq6: 100%|██████████| 91/91 [00:17<00:00,  5.27it/s]\n","Processing seq14: 100%|██████████| 249/249 [00:47<00:00,  5.28it/s]\n","Processing seq11: 100%|██████████| 228/228 [00:32<00:00,  7.09it/s]\n"]}],"source":["\n","###############################\n","####### Inference Loop ########\n","###############################\n","new_size=(256,256)\n","discontinuity_threshold=1 * 10**(-2)\n","percentuale=0.6\n","# Loop over the subjects and their images\n","for subject, img_list in seq_dict.items():\n","    # Create a folder for each subject in the results folder\n","    subject_folder = os.path.join(results_folder,subject)\n","    if not os.path.exists(subject_folder):\n","        os.makedirs(subject_folder)\n","\n","    ### NB: if you want to apply preprocessing on the 3D volume you need to reconstruct\n","    # it here and change the loop to cycle over the slices of the volume instead of\n","    # cycling over the images\n","\n","    for img_name in tqdm(img_list, desc=f'Processing {subject}'):\n","        if img_name.endswith('.jpg'):\n","            img_path = os.path.join(test_img_folder, img_name)\n","\n","            ###############################\n","            ##### 2D Pre-Processing #######\n","            ###############################\n","\n","\n","            # Carica l'immagine a colori (RGB)\n","            image = Image.open(img_path)\n","            if image is None:\n","              print(f\"Errore nel caricamento dell'immagine: {img_path}\")\n","\n","\n","\n","            dimensioni=np.shape(image)\n","            # Ridimensiona l'immagine risultante a 256x256\n","            image1=image.resize(new_size)\n","            corrected_image=regional_masking_and_inpainting(image1, discontinuity_threshold)\n","\n","            corrected_image_pil = Image.fromarray(img_as_ubyte(corrected_image))\n","\n","\n","\n","            # Save img to temporary file\n","            corrected_image_pil.save('tmp_img.png')\n","\n","            image_modified=normalizza_immagine('tmp_img.png')\n","\n","            image_modified.save('tmp_img.png')\n","\n","\n","\n","\n","            ###############################\n","            #######    Inference   ########\n","            ###############################\n","\n","\n","            result = inference_model(model, 'tmp_img.png')\n","\n","            # Get data from the result\n","            pred_label = result.pred_sem_seg.data.squeeze()\n","            pred_label = pred_label.cpu().numpy().astype(np.uint8)\n","            height, width = pred_label.shape[:2]  # Ottieni altezza e larghezza della maschera\n","            min_size = (height * width) * (percentuale / 100)  # Calcola l'area minima\n","\n","            ###############################\n","            ###### 2D Post-Processing #####\n","            ###############################\n","\n","            # Filtra le regioni connesse\n","            filtered_mask = remove_small_regions(pred_label, min_size=min_size)\n","\n","\n","\n","            ###############################\n","            #######   Save results  #######\n","            ###############################\n","\n","\n","\n","\n","            # Save the result\n","            pred_label_img = Image.fromarray(filtered_mask, mode='L')\n","            #pred_label_img = Image.fromarray(pred_label, mode='L')\n","            pred_label_img = pred_label_img.resize((dimensioni[1],dimensioni[0]))\n","            pred_label_img.save(os.path.join(subject_folder, os.path.basename(img_name).replace(\".jpg\",\".png\")))\n"]},{"cell_type":"code","source":["#AGGIUNGE POLIPI\n","\n","# Imposta i percorsi\n","results_folder = results_folder\n","output_folder = results_folder\n","os.makedirs(output_folder, exist_ok=True)  # Crea la cartella se non esiste\n","\n","\n","# Funzione per aggiungere un polipo basandosi sulle maschere adiacenti\n","def add_polyp_between_adjacent_frames(prev_mask, curr_mask, next_mask):\n","    if np.max(curr_mask) == 0 and np.max(prev_mask) > 0 and np.max(next_mask) > 0:\n","        intersection_mask = np.logical_and(prev_mask > 0, next_mask > 0)\n","        if np.any(intersection_mask):\n","            curr_mask[intersection_mask] = 1\n","    return curr_mask\n","\n","# Funzione per elaborare tutte le maschere di una sequenza\n","def process_masks_in_sequence(results_folder,seq,sequence_images, output_folder, percentuale=1):\n","    if len(sequence_images) < 3:\n","        print(f\"Sequenza troppo corta ({len(sequence_images)} immagini) per essere elaborata.\")\n","        return\n","\n","    for i in tqdm(range(len(sequence_images)), desc=\"Processing masks\", unit=\"mask\"):\n","        try:\n","            # Percorso della maschera corrente\n","            curr_path = sequence_images[i]\n","\n","\n","            # Ricava solo la base\n","            base = os.path.basename(curr_path)\n","\n","             # Carica la maschera corrente\n","            curr_mask = np.array(Image.open(os.path.join(results_folder,seq,base.replace('.jpg','.png'))))\n","\n","            # Calcola l'area minima per la rimozione delle piccole regioni\n","            height, width = curr_mask.shape[:2]\n","            min_size = (height * width) * (percentuale / 100)\n","\n","            if i == 0 or i == len(sequence_images) - 1:\n","                # Prima o ultima immagine: solo rimozione di piccole regioni\n","                continue\n","            else:\n","                # Immagini centrali: rimozione piccole regioni + aggiunta polipi\n","                base_i_meno_1 = os.path.basename(sequence_images[i - 1])\n","                prev_mask = np.array(Image.open(os.path.join(results_folder,seq,base_i_meno_1.replace('.jpg','.png'))))\n","                base_i_piu_1 = os.path.basename(sequence_images[i + 1])\n","                next_mask = np.array(Image.open(os.path.join(results_folder,seq,base_i_piu_1.replace('.jpg','.png'))))\n","\n","                processed_mask = add_polyp_between_adjacent_frames(prev_mask, curr_mask, next_mask)\n","\n","            # Salva la maschera aggiornata\n","            relative_path = os.path.join(seq, base.replace('.jpg','.png'))\n","            output_path = os.path.join(output_folder, relative_path)\n","            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","            Image.fromarray(processed_mask).save(output_path)\n","        except Exception as e:\n","            print(f\"Errore durante l'elaborazione dell'immagine: {curr_path}\")\n","            print(f\"Dettagli dell'errore: {e}\")\n","\n","# Itera su tutte le sequenze nel dizionario\n","for seq, sequence_images in seq_dict.items():\n","    print(f\"Processing sequence: {seq}\")\n","    process_masks_in_sequence(results_folder,seq,sequence_images, output_folder, percentuale=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S34Vfyn6UMQB","executionInfo":{"status":"ok","timestamp":1736865319084,"user_tz":-60,"elapsed":22044,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"d41be818-e935-47b0-9622-dee304d50403"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing sequence: seq6\n"]},{"output_type":"stream","name":"stderr","text":["Processing masks: 100%|██████████| 91/91 [00:04<00:00, 18.91mask/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing sequence: seq14\n"]},{"output_type":"stream","name":"stderr","text":["Processing masks: 100%|██████████| 249/249 [00:10<00:00, 23.53mask/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing sequence: seq11\n"]},{"output_type":"stream","name":"stderr","text":["Processing masks: 100%|██████████| 228/228 [00:06<00:00, 36.63mask/s]\n"]}]},{"cell_type":"code","source":["# Lista delle sequenze (sub-cartelle)\n","sequences = os.listdir(gt_mask_folder)\n","\n","# Dizionario per salvare i risultati per sequenza\n","evaluation_results = {}\n","\n","# Calcola le metriche per tutte le immagini\n","for seq in sequences:\n","  # Costruisci i percorsi per le sottocartelle\n","  gt_seq_folder = os.path.join(gt_mask_folder, seq)\n","  result_seq_folder = os.path.join(results_folder, seq)\n","\n","  # Ordinare le liste di maschere manuali e automatiche\n","  list_mask_manual = sorted(os.listdir(gt_seq_folder), key=extract_last_number_from_filename)\n","  list_mask_auto = sorted(os.listdir(result_seq_folder), key=extract_last_number_from_filename)\n","\n","  # Assicurati che la lista delle maschere sia la stessa in entrambe le cartelle\n","  if len(list_mask_manual) != len(list_mask_auto):\n","      raise ValueError(f\"Il numero di maschere nelle cartelle {gt_seq_folder} e {result_seq_folder} non corrispondono!\")\n","\n","  # Liste per raccogliere le metriche per questa sequenza\n","  Dices = []\n","  absolute_errors = []\n","  TSIs = []  # Lista per raccogliere i valori di TSI\n","\n","  # Lista per raccogliere le metriche per ogni immagine\n","  images_metrics = []\n","\n","  # Ciclo su ogni immagine e calcola le metriche\n","  prev_auto_mask = None  # Maschera automatica precedente per il calcolo del TSI\n","  for manual_mask_path, auto_mask_path in tqdm(zip(list_mask_manual, list_mask_auto)):\n","      # Rimuoviamo \"_mask\" dal nome del file per fare il confronto\n","      if \"_mask\" in manual_mask_path:\n","          manual_mask_name = manual_mask_path.replace(\"_mask\", \"\")\n","      else:\n","          manual_mask_name = manual_mask_path\n","\n","      # Verifica che i nomi corrispondano tra ground truth e risultato\n","      if manual_mask_name.replace(\".jpg\",\"\") != auto_mask_path.replace(\".png\",\"\"):\n","          raise ValueError(f\"Il nome della maschera nel ground truth ({manual_mask_path}) non corrisponde con il nome nel risultato ({auto_mask_path})!\")\n","\n","      # Lettura delle maschere\n","      manual_mask = Image.open(os.path.join(gt_seq_folder, manual_mask_path))\n","      auto_mask = Image.open(os.path.join(result_seq_folder, auto_mask_path))\n","\n","      auto_mask = np.array(auto_mask)\n","      manual_mask = np.array(manual_mask.convert('L'))\n","      manual_mask = np.where(manual_mask > 127, 1, 0).astype(np.uint8)\n","\n","      # Calcola il Dice coefficient\n","      Dice = calculate_2d_dice(manual_mask, auto_mask)\n","\n","      # Aggiungi il Dice alla lista\n","      Dices.append(Dice)\n","\n","      # Conta gli oggetti connessi\n","      pred_count = count_connected_objects(auto_mask)\n","      gt_count = count_connected_objects(manual_mask)\n","\n","      # Calcola l'errore assoluto\n","      absolute_error = calculate_absolute_error(pred_count, gt_count)\n","      absolute_errors.append(absolute_error)\n","\n","      # Calcola il TSI se esiste una maschera precedente\n","      if prev_auto_mask is not None:\n","          tsi = calculate_tsi(prev_auto_mask, auto_mask)\n","          TSIs.append(tsi)\n","      else:\n","          tsi = None  # Nessun valore TSI per il primo frame\n","\n","      # Aggiorna la maschera precedente\n","      prev_auto_mask = auto_mask\n","\n","      # Aggiungi le metriche di questa immagine alla lista per questa sequenza\n","      images_metrics.append({\n","          'image': manual_mask_name,  # Nome dell'immagine\n","          'Dice': float(Dice),\n","          'Absolute Error': float(absolute_error),\n","          'TSI': float(tsi) if tsi is not None else None\n","      })\n","\n","  # Calcola media e deviazione standard per le metriche per la sequenza\n","  evaluation_results[seq] = {\n","      'metrics': {\n","          'Dice': {\n","              'mean': float(np.mean(Dices)),\n","              'std': float(np.std(Dices))\n","          },\n","          'Absolute Error': {\n","              'mean': float(np.mean(absolute_errors)),\n","              'std': float(np.std(absolute_errors))\n","          },\n","          'TSI': {\n","              'mean': float(np.mean(TSIs)) if TSIs else None,\n","              'std': float(np.std(TSIs)) if TSIs else None\n","          }\n","      },\n","      'images': images_metrics  # Aggiungi le metriche per ogni immagine\n","  }\n","\n","# Stampa i risultati\n","for seq, metrics in evaluation_results.items():\n","  print(f\"\\nRisultati per la sequenza {seq}:\")\n","  for metric, values in metrics['metrics'].items():\n","      print(f\"  {metric} - Media: {values['mean']:.4f}, Deviazione standard: {values['std']:.4f}\")\n","\n","# Salva i risultati dell'evaluazione su un file JSON\n","evaluation_json_path = os.path.join(results_folder, 'performance_dice_per_paziente.json')\n","with open(evaluation_json_path, 'w') as json_file:\n","  json.dump(evaluation_results, json_file, indent=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1dovqJbYn5u","executionInfo":{"status":"ok","timestamp":1736865343707,"user_tz":-60,"elapsed":21421,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"outputId":"f0c07358-9617-4d42-810f-df9bfc4e068d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["91it [00:04, 18.68it/s]\n","249it [00:12, 20.27it/s]\n","228it [00:03, 58.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Risultati per la sequenza seq6:\n","  Dice - Media: 0.7191, Deviazione standard: 0.4026\n","  Absolute Error - Media: 0.3516, Deviazione standard: 0.6525\n","  TSI - Media: 0.0937, Deviazione standard: 0.2285\n","\n","Risultati per la sequenza seq14:\n","  Dice - Media: 0.6141, Deviazione standard: 0.3973\n","  Absolute Error - Media: 2.5663, Deviazione standard: 6.2515\n","  TSI - Media: 0.3001, Deviazione standard: 0.3660\n","\n","Risultati per la sequenza seq11:\n","  Dice - Media: 0.5183, Deviazione standard: 0.4537\n","  Absolute Error - Media: 0.4079, Deviazione standard: 0.5003\n","  TSI - Media: 0.0401, Deviazione standard: 0.1261\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Leggi il file JSON\n","with open(os.path.join(results_folder,'performance_dice_per_paziente.json'), 'r') as file:\n","    data = json.load(file)\n","\n","# Inizializza dizionari per salvare i valori di tutte le metriche globalmente\n","metrics = {\"Dice\": [], \"Absolute Error\": [], \"TSI\": []}\n","\n","# Itera su tutte le sequenze nel file JSON\n","for seq_name, seq_data in data.items():\n","    # Estrarre le immagini e le loro metriche\n","    images = seq_data['images']\n","\n","    # Popola i dizionari con i valori per ogni immagine\n","    for img in images:\n","        for metric in metrics:\n","            value = img.get(metric)\n","            if value is not None:\n","                metrics[metric].append(value)\n","\n","# Calcola la media e la deviazione standard globali per ciascuna metrica\n","overall_stats = {}\n","for metric, values in metrics.items():\n","    if values:  # Verifica che ci siano valori per la metrica\n","        mean = np.mean(values)\n","        std = np.std(values)\n","        overall_stats[metric] = {\"mean\": mean, \"std\": std}\n","\n","# Stampa i risultati globali\n","print(\"Statistiche globali per: \")\n","for metric, stats in overall_stats.items():\n","    print(f\"{metric}: Mean = {stats['mean']:.4f}, Std = {stats['std']:.4f}\")\n"],"metadata":{"id":"Eat0mEwxP_55","executionInfo":{"status":"ok","timestamp":1736865343708,"user_tz":-60,"elapsed":6,"user":{"displayName":"multiscala 5","userId":"13733666679229301804"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cbe2f822-13cf-497d-9b5d-a9406837fde6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Statistiche globali per: \n","Dice: Mean = 0.5925, Std = 0.4275\n","Absolute Error: Mean = 1.3451, Std = 4.2971\n","TSI: Mean = 0.1628, Std = 0.2977\n"]}]}],"metadata":{"colab":{"cell_execution_strategy":"setup","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}